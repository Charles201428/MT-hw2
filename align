#!/usr/bin/env python
import numpy as np
import pickle as pk

import sys
import os
import optparse
import logging

def parse():
    optparser = optparse.OptionParser()
    optparser.add_option("-d", "--data", dest="train", default="data/hansards", help="Data filename prefix (default=data)")
    optparser.add_option("-e", "--english", dest="english", default="e", help="Suffix of English filename (default=e)")
    optparser.add_option("-f", "--french", dest="french", default="f", help="Suffix of French filename (default=f)")
    optparser.add_option("--eps", dest="eps", default=1e-3, type="float", help="Default error bound for stopping the EM algorithm (default=1e-3)")
    optparser.add_option("-n", "--num_sentences", dest="num_sents", default=100000000000, type="int", help="Number of sentences to use for training and alignment")
    optparser.add_option("-l", "--lower", action="store_true", dest="lower", help="Convert corpus to lower case (defualt=False)")
    optparser.add_option("-g", "--debugging", action="store_true", dest="debugging", help="Turn on debugging mode (defualt=False)")
    optparser.add_option("-s", "--save", dest="save_path", default="ibm1.pkl", help="Model save path")
    optparser.add_option("-p", "--pretrained", dest="load_path", default="", help="Model save path")
    optparser.add_option("-m", "--max_iterations", dest="max_iters", default=500, type="int", help="Max number of iterations to train")
    optparser.add_option("-c", "--continue", dest="iter_now", type=int, default=None, help="Continue with iteration number provided by this option. Need to have -p option turned on")
    (opts, _) = optparser.parse_args()

    french = f"{opts.train}.{opts.french}"
    english = f"{opts.train}.{opts.english}"

    test(opts)

    return opts, french, english



def test (a) -> None:
    if a.iter_now is not None:
        assert a.load_path
        
    if not a.load_path:
        c, d = os.path.splitext(a.save_path)
        c = f"{c}_num{a.num_sents}_eps{a.eps}"
        e = c + d
        f, g = os.path.split(e)
        f = os.path.join(f, c)
        assert not os.path.exists(f), f"{f} occupied."
        os.makedirs(f)
        a.save_path = os.path.join(f, g)






class Tokenizer:
    def __init__(self, corpus, case_sensitive=True) -> None:
        words_set = set()
        for inpu in corpus:
            for choice in inpu:
                if case_sensitive:
                    words_set.add(choice)
                else:
                    choice = choice.lower()
                    words_set.add(choice)
        self.toWord = list(words_set)
        self.toIndex = {choice:index for index, choice in enumerate(self.toWord)}
        self.V = len(self.toWord)
    
    def encode(self, inpu):

        encoderesult = [self.toIndex[word] for word in inpu]

        return encoderesult
    
    def decode(self, inpuIndex):

        decoderesult = [self.toWord[id] for id in inpuIndex]   
        return decoderesult


class IBMModel1:
    def __init__(self, bitext, f_tokenizer, e_tokenizer, save_path, eps=1e-5, max_iters=500) -> None:
        '''
        For all given f, \sum_e t(e|f) = 1

        Suppose Vocabulary size of e and f are Ve and Vf.
        Use a Vf*Ve matrix to represent t(e|f), where the rows are different f
        and the columns are different e. Thus, each row sum to 1.
        '''
        bitext_tokenized = [[f_tokenizer.encode(sent[0]), e_tokenizer.encode(sent[1])] for sent in bitext]
        self.bitext = bitext_tokenized
        '''Integerized parallel corpus'''
        self.f_tokenizer = f_tokenizer
        self.e_tokenizer = e_tokenizer
        self.Vf = f_tokenizer.V
        self.Ve = e_tokenizer.V
        self.t = np.ones((self.Vf, self.Ve)) / self.Ve # so that each entry is uniformly 1 / Ve (row normalized)
        self.save_path = save_path

        self.eps = eps
        self.max_iters = max_iters
        self.is_converged = False
        self.diff = -1
    
    def save(self, iters):
        base, ext = os.path.splitext(opts.save_path)
        base = f"{base}_iter{iters}"
        path = base + ext
        pk.dump(self, open(path, "wb"))

    def em_train(self, iter_now=0):
        iters = iter_now
        while (iters < self.max_iters) and (not self.is_converged):
            iters += 1
            logging.info(f"Iteration {iters}, diff={self.diff:.8f}")
            logging.debug(f"t(the|la)={self.t_e_f('the', 'la')}")
            logging.debug(f"t(of|la)={self.t_e_f('of', 'la')}")

            # initialize
            count = np.zeros((self.Vf, self.Ve))
            total = np.zeros(self.Vf)

            for f_sent, e_sent in self.bitext:
                # compute normalization
                s_total = {}
                for e in e_sent:
                    s_total[e] = 0
                    for f in f_sent:
                        s_total[e] += self.t[f, e]
                # collect counts
                for e in e_sent:
                    for f in f_sent:
                        count[f, e] += self.t[f, e]/s_total[e]
                        total[f] += self.t[f, e]/s_total[e]
            
            # estimate probabilities
            self.diff = 0
            for f in range(self.Vf):
                for e in range(self.Ve):
                    new_t = count[f, e] / total[f]
                    el_diff = np.abs(new_t - self.t[f, e])
                    self.diff = max(self.diff, el_diff)
                    self.t[f, e] = new_t

            # check convergence and save
            if self.diff < self.eps:
                self.is_converged = True
            self.save(iters)

        if self.is_converged:
            logging.info(f"Model converged after {iters} iteration under error {self.eps}!")
        else:
            logging.info(f"Training stopped after reaching max number of iterations {self.max_iters}")

    def t_e_f(self, e, f):
        '''
        Computes t(e|f), where e & f are both string words
        
        If either e or f is not in the vocabulary, return -1
        '''
        try:
            ei = self.e_tokenizer.w2i[e]
            fi = self.f_tokenizer.w2i[f]
        except KeyError:
            return -1
        return self.t[fi, ei]

def align(model, bitext):
    for (f, e) in bitext:
        for (i, f_i) in enumerate(f): 
            max_prob = 0
            best_j = 0
            for (j, e_j) in enumerate(e):
                if model.t_e_f(e_j, f_i) > max_prob:
                    best_j = j
                    max_prob = model.t_e_f(e_j, f_i)
            if(max_prob > 0):
                sys.stdout.write("%i-%i " % (i,best_j))
        sys.stdout.write("\n")

if __name__ == "__main__":
    opts, f_data, e_data = parse()
    loglvl = logging.DEBUG if opts.debugging else logging.INFO
    logging.basicConfig(level=loglvl)
    bitext = [[sentence.strip().split() for sentence in pair] for pair in zip(open(f_data), open(e_data))][:opts.num_sents]
    if opts.lower:
        bitext = [[[w.lower() for w in fs],[w.lower() for w in es]] for fs, es in bitext]
    f_text = [sent[0] for sent in bitext]
    e_text = [sent[1] for sent in bitext]
    f_tokenizer = Tokenizer(f_text)
    e_tokenizer = Tokenizer(e_text)
    # bitext_tokenized = []
    # for sent in bitext:
    #     print(sent)
    #     bitext_tokenized.append([f_tokenizer.encode(sent[0]), e_tokenizer.encode(sent[1])])

    logging.info(f"Vocabulary size - foreign:{f_tokenizer.V}, english:{e_tokenizer.V}")
    if opts.load_path:
        ibm = pk.load(open(opts.load_path, "rb"))
        if opts.iter_now is not None:
            ibm.em_train(opts.iter_now)
    else:
        ibm = IBMModel1(bitext, f_tokenizer, e_tokenizer, opts.save_path, eps=opts.eps, max_iters=opts.max_iters)
        ibm.em_train()
    align(ibm, bitext)