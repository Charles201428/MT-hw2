{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfa8640e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import optparse\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "\n",
    "def get_args():\n",
    "    parser = optparse.OptionParser()\n",
    "    parser.add_option(\"-d\", \"--data\", dest=\"data_prefix\", default=\"data/hansards\", help=\"Data prefix\")\n",
    "    parser.add_option(\"-e\", \"--english\", dest=\"english_suffix\", default=\"e\")\n",
    "    parser.add_option(\"-f\", \"--french\", dest=\"french_suffix\", default=\"f\")\n",
    "    parser.add_option(\"--eps\", dest=\"epsilon\", default=1e-3, type=\"float\")\n",
    "    parser.add_option(\"-n\", \"--num_sentences\", dest=\"sentence_count\", default=1e12, type=\"int\")\n",
    "    parser.add_option(\"-l\", \"--lower\", action=\"store_true\", dest=\"to_lower\")\n",
    "    parser.add_option(\"-g\", \"--debug\", action=\"store_true\", dest=\"debug_mode\")\n",
    "    parser.add_option(\"-s\", \"--save\", dest=\"save_loc\", default=\"ibm1.pkl\")\n",
    "    parser.add_option(\"-p\", \"--load\", dest=\"load_loc\", default=\"\")\n",
    "    parser.add_option(\"-m\", \"--max_iterations\", dest=\"max_iters\", default=500, type=\"int\")\n",
    "    parser.add_option(\"-c\", \"--continue_from\", dest=\"continue_iter\", type=int, default=None)\n",
    "    options, _ = parser.parse_args()\n",
    "\n",
    "    f_filepath = f\"{options.data_prefix}.{options.french_suffix}\"\n",
    "    e_filepath = f\"{options.data_prefix}.{options.english_suffix}\"\n",
    "\n",
    "    return options, f_filepath, e_filepath\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, text_data, sensitive=True):\n",
    "        words = {word.lower() if not sensitive else word for sentence in text_data for word in sentence}\n",
    "        self.index_to_word = list(words)\n",
    "        self.word_to_index = {word: idx for idx, word in enumerate(self.index_to_word)}\n",
    "        self.size = len(self.index_to_word)\n",
    "\n",
    "    def encode(self, sentence):\n",
    "        return [self.word_to_index[word] for word in sentence]\n",
    "\n",
    "    def decode(self, indices):\n",
    "        return [self.index_to_word[i] for i in indices]\n",
    "    \n",
    "    \n",
    "\n",
    "class IBM1:\n",
    "    def __init__(self, paired_text, french_vocab, english_vocab, save_path, eps=1e-5, max_iter=500):\n",
    "        self.paired_text = [[french_vocab.encode(f), english_vocab.encode(e)] for f, e in paired_text]\n",
    "        self.french_vocab = french_vocab\n",
    "        self.english_vocab = english_vocab\n",
    "        self.prob_matrix = np.ones((french_vocab.size, english_vocab.size)) / english_vocab.size\n",
    "        self.save_path = save_path\n",
    "        self.eps = eps\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "    def save_model(self, iteration):\n",
    "        # Save the current state of the model to a file\n",
    "        with open(f\"{self.save_path}_iter{iteration}.pkl\", \"wb\") as file:\n",
    "            pk.dump(self, file)\n",
    "            \n",
    "            \n",
    "    def train(self):\n",
    "        for iteration in range(self.max_iter):\n",
    "            count = np.zeros((self.french_vocab.size, self.english_vocab.size))\n",
    "            total = np.zeros(self.french_vocab.size)\n",
    "            \n",
    "            for french_sent, english_sent in self.paired_text:\n",
    "                s_total = {e: 0 for e in english_sent}\n",
    "                for e in english_sent:\n",
    "                    for f in french_sent:\n",
    "                        s_total[e] += self.prob_matrix[f, e]\n",
    "                \n",
    "                for e in english_sent:\n",
    "                    for f in french_sent:\n",
    "                        count[f, e] += self.prob_matrix[f, e] / s_total[e]\n",
    "                        total[f] += self.prob_matrix[f, e] / s_total[e]\n",
    "            \n",
    "            diff = 0\n",
    "            for f in range(self.french_vocab.size):\n",
    "                for e in range(self.english_vocab.size):\n",
    "                    new_prob = count[f, e] / total[f]\n",
    "                    diff = max(diff, abs(new_prob - self.prob_matrix[f, e]))\n",
    "                    self.prob_matrix[f, e] = new_prob\n",
    "\n",
    "            if diff < self.eps:\n",
    "                break\n",
    "                    \n",
    "    def probability(self, e_word, f_word):\n",
    "        e_idx = self.english_vocab.word_to_index.get(e_word, -1)\n",
    "        f_idx = self.french_vocab.word_to_index.get(f_word, -1)\n",
    "        return -1 if e_idx == -1 or f_idx == -1 else self.prob_matrix[f_idx, e_idx]\n",
    "    \n",
    "def alignment(model, paired_text):\n",
    "    for french, english in paired_text:\n",
    "        for i, f_word in enumerate(french): \n",
    "            best_j, max_prob = 0, 0\n",
    "            for j, e_word in enumerate(english):\n",
    "                current_prob = model.probability(e_word, f_word)\n",
    "                if current_prob > max_prob:\n",
    "                    best_j, max_prob = j, current_prob\n",
    "            if max_prob > 0:\n",
    "                sys.stdout.write(f\"{i}-{best_j} \")\n",
    "        sys.stdout.write(\"\\n\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    options, french_data_path, english_data_path = get_args()\n",
    "    log_level = logging.DEBUG if options.debug_mode else logging.INFO\n",
    "    logging.basicConfig(level=log_level)\n",
    "\n",
    "    data = [[sent.strip().split() for sent in pair] for pair in zip(open(french_data_path), open(english_data_path))][:options.sentence_count]\n",
    "    if options.to_lower:\n",
    "        data = [[w.lower() for w in sentence] for sentence in pair for pair in data]\n",
    "\n",
    "    french_vocab = Vocabulary([sentence[0] for sentence in data])\n",
    "    english_vocab = Vocabulary([sentence[1] for sentence in data])\n",
    "\n",
    "    logging.info(f\"French Vocab Size: {french_vocab.size}, English Vocab Size: {english_vocab.size}\")\n",
    "\n",
    "    model = None\n",
    "    if options.load_loc:\n",
    "        model = pk.load(open(options.load_loc, \"rb\"))\n",
    "        if options.continue_iter:\n",
    "            model.train_continue(options.continue_iter)\n",
    "    else:\n",
    "        model = IBM1(data, french_vocab, english_vocab, options.save_loc, eps=options.epsilon, max_iter=options.max_iters)\n",
    "        model.train()\n",
    "\n",
    "    alignment(model, data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
