{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfa8640e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import optparse\n",
    "import logging\n",
    "\n",
    "def parse():\n",
    "    optparser = optparse.OptionParser()\n",
    "    optparser.add_option(\"-d\", \"--data\", dest=\"train\", default=\"data/hansards\", help=\"Data filename prefix (default=data)\")\n",
    "    optparser.add_option(\"-e\", \"--english\", dest=\"english\", default=\"e\", help=\"Suffix of English filename (default=e)\")\n",
    "    optparser.add_option(\"-f\", \"--french\", dest=\"french\", default=\"f\", help=\"Suffix of French filename (default=f)\")\n",
    "    optparser.add_option(\"--eps\", dest=\"eps\", default=1e-3, type=\"float\", help=\"Default error bound for stopping the EM algorithm (default=1e-3)\")\n",
    "    optparser.add_option(\"-n\", \"--num_sentences\", dest=\"num_sents\", default=100000000000, type=\"int\", help=\"Number of sentences to use for training and alignment\")\n",
    "    optparser.add_option(\"-l\", \"--lower\", action=\"store_true\", dest=\"lower\", help=\"Convert corpus to lower case (defualt=False)\")\n",
    "    optparser.add_option(\"-g\", \"--debugging\", action=\"store_true\", dest=\"debugging\", help=\"Turn on debugging mode (defualt=False)\")\n",
    "    optparser.add_option(\"-s\", \"--save\", dest=\"save_path\", default=\"ibm1.pkl\", help=\"Model save path\")\n",
    "    optparser.add_option(\"-p\", \"--pretrained\", dest=\"load_path\", default=\"\", help=\"Model save path\")\n",
    "    optparser.add_option(\"-m\", \"--max_iterations\", dest=\"max_iters\", default=500, type=\"int\", help=\"Max number of iterations to train\")\n",
    "    optparser.add_option(\"-c\", \"--continue\", dest=\"iter_now\", type=int, default=None, help=\"Continue with iteration number provided by this option. Need to have -p option turned on\")\n",
    "    (opts, _) = optparser.parse_args()\n",
    "\n",
    "    french = f\"{opts.train}.{opts.french}\"\n",
    "    english = f\"{opts.train}.{opts.english}\"\n",
    "\n",
    "    test(opts)\n",
    "\n",
    "    return opts, french, english\n",
    "\n",
    "\n",
    "\n",
    "def test (a) -> None:\n",
    "    if a.iter_now is not None:\n",
    "        assert a.load_path\n",
    "        \n",
    "    if not a.load_path:\n",
    "        c, d = os.path.splitext(a.save_path)\n",
    "        c = f\"{c}_num{a.num_sents}_eps{a.eps}\"\n",
    "        e = c + d\n",
    "        f, g = os.path.split(e)\n",
    "        f = os.path.join(f, c)\n",
    "        assert not os.path.exists(f), f\"{f} occupied.\"\n",
    "        os.makedirs(f)\n",
    "        a.save_path = os.path.join(f, g)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, corpus, case_sensitive=True) -> None:\n",
    "        words_set = set()\n",
    "        for inpu in corpus:\n",
    "            for choice in inpu:\n",
    "                if case_sensitive:\n",
    "                    words_set.add(choice)\n",
    "                else:\n",
    "                    choice = choice.lower()\n",
    "                    words_set.add(choice)\n",
    "        self.toWord = list(words_set)\n",
    "        self.toIndex = {choice:index for index, choice in enumerate(self.toWord)}\n",
    "        self.V = len(self.toWord)\n",
    "    \n",
    "    def encode(self, inpu):\n",
    "\n",
    "        encoderesult = [self.toIndex[word] for word in inpu]\n",
    "\n",
    "        return encoderesult\n",
    "    \n",
    "    def decode(self, inpuIndex):\n",
    "\n",
    "        decoderesult = [self.toWord[id] for id in inpuIndex]   \n",
    "        return decoderesult\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class IBMModel1:\n",
    "    def __init__(self, bilingual_text, source_tokenizer, target_tokenizer, save_directory, tolerance=1e-5, max_iterations=50):\n",
    "        tokenized_text = [[source_tokenizer.encode(pair[0]), target_tokenizer.encode(pair[1])] for pair in bilingual_text]\n",
    "        self.bilingual_text = tokenized_text\n",
    "        self.source_tokenizer = source_tokenizer\n",
    "        self.target_tokenizer = target_tokenizer\n",
    "        self.source_vocab_size = source_tokenizer.V\n",
    "        self.target_vocab_size = target_tokenizer.V\n",
    "        self.translation_prob = np.ones((self.source_vocab_size, self.target_vocab_size)) / self.target_vocab_size\n",
    "        self.save_directory = save_directory\n",
    "\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iterations = max_iterations\n",
    "        self.has_converged = False\n",
    "        self.change = -1\n",
    "    \n",
    "    def save(self, iteration_count):\n",
    "        base_name, file_extension = os.path.splitext(o.save_path)\n",
    "        base_name = f\"{base_name}_iter{iteration_count}\"\n",
    "        path = base_name + file_extension\n",
    "        pk.dump(self, open(path, \"wb\"))\n",
    "\n",
    "    def train(self, current_iteration=0):\n",
    "        iteration = current_iteration\n",
    "        while (iteration < self.max_iterations) and (not self.has_converged):\n",
    "            iteration += 1\n",
    "            logging.info(f\"Iteration {iteration}, change={self.change:.8f}\")\n",
    "            logging.debug(f\"translation_prob(the|la)={self.get_prob('the', 'la')}\")\n",
    "            logging.debug(f\"translation_prob(of|la)={self.get_prob('of', 'la')}\")\n",
    "\n",
    "            count_matrix = np.zeros((self.source_vocab_size, self.target_vocab_size))\n",
    "            total_vector = np.zeros(self.source_vocab_size)\n",
    "\n",
    "            for source_sentence, target_sentence in self.bilingual_text:\n",
    "                sentence_total = {}\n",
    "                for target_word in target_sentence:\n",
    "                    sentence_total[target_word] = 0\n",
    "                    for source_word in source_sentence:\n",
    "                        sentence_total[target_word] += self.translation_prob[source_word, target_word]\n",
    "\n",
    "                for target_word in target_sentence:\n",
    "                    for source_word in source_sentence:\n",
    "                        count_matrix[source_word, target_word] += self.translation_prob[source_word, target_word]/sentence_total[target_word]\n",
    "                        total_vector[source_word] += self.translation_prob[source_word, target_word]/sentence_total[target_word]\n",
    "            \n",
    "            self.change = 0\n",
    "            for source_word_idx in range(self.source_vocab_size):\n",
    "                for target_word_idx in range(self.target_vocab_size):\n",
    "                    updated_prob = count_matrix[source_word_idx, target_word_idx] / total_vector[source_word_idx]\n",
    "                    element_change = np.abs(updated_prob - self.translation_prob[source_word_idx, target_word_idx])\n",
    "                    self.change = max(self.change, element_change)\n",
    "                    self.translation_prob[source_word_idx, target_word_idx] = updated_prob\n",
    "\n",
    "            if self.change < self.tolerance:\n",
    "                self.has_converged = True\n",
    "            self.save(iteration)\n",
    "\n",
    "        if self.has_converged:\n",
    "            logging.info(f\"Model converged after {iteration} iteration under error {self.tolerance}!\")\n",
    "        else:\n",
    "            logging.info(f\"Training stopped after reaching max number of iterations {self.max_iterations}\")\n",
    "\n",
    "    def get_prob(self, target_word, source_word):\n",
    "        try:\n",
    "            target_index = self.target_tokenizer.toIndex[target_word]\n",
    "            source_index = self.source_tokenizer.toIndex[source_word]\n",
    "        except KeyError:\n",
    "            return -1\n",
    "        return self.translation_prob[source_index, target_index]\n",
    "\n",
    "\n",
    "def align(translation_model, bilingual_text):\n",
    "    for (source_sentence, target_sentence) in bilingual_text:\n",
    "        for (source_index, source_word) in enumerate(source_sentence): \n",
    "            highest_probability = 0\n",
    "            best_target_index = 0\n",
    "            for (target_index, target_word) in enumerate(target_sentence):\n",
    "                if translation_model.t_e_f(target_word, source_word) > highest_probability:\n",
    "                    best_target_index = target_index\n",
    "                    highest_probability = translation_model.t_e_f(target_word, source_word)\n",
    "            if(highest_probability > 0):\n",
    "                sys.stdout.write(\"%i-%i \" % (source_index, best_target_index))\n",
    "        sys.stdout.write(\"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    o, fd, ed = parse()\n",
    "    loglvl = logging.DEBUG if o.debugging else logging.INFO\n",
    "    logging.basicConfig(level=loglvl)\n",
    "    \n",
    "    b = []\n",
    "    for p in zip(open(fd), open(ed)):\n",
    "        temp = []\n",
    "        for s in p:\n",
    "            temp.append(s.strip().split())\n",
    "        b.append(temp)\n",
    "            \n",
    "            \n",
    "            \n",
    "    if o.lower:\n",
    "        b = [[[word.lower() for word in fst],[word.lower() for word in est]] for fst, est in b]\n",
    "    ft = [Input[0] for Input in b]\n",
    "    et = [Input[1] for Input in b]\n",
    "    f_tokenizer = Tokenizer(ft)\n",
    "    e_tokenizer = Tokenizer(et)\n",
    "\n",
    "    logging.info(f\"Vocabulary size - foreign:{f_tokenizer.V}, english:{e_tokenizer.V}\")\n",
    "    if o.load_path:\n",
    "        model = pk.load(open(o.load_path, \"rb\"))\n",
    "        if o.iter_now is not None:\n",
    "            model.train(o.iter_now)\n",
    "    else:\n",
    "        model = IBMModel1(b, f_tokenizer, e_tokenizer, o.save_path, tolerance=o.eps, max_iterations=o.max_iters)\n",
    "        model.train()\n",
    "    align(model, b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
