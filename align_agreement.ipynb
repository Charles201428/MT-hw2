{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88e449e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "ibm1_num100000000000_eps0.001 already exists. Aborting.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19024/1688771724.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m     \u001b[0mopts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[0mloglvl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEBUG\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mopts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebugging\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasicConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloglvl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19024/1688771724.py\u001b[0m in \u001b[0;36mparse_args\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mfolder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"{folder} already exists. Aborting.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: ibm1_num100000000000_eps0.001 already exists. Aborting."
     ]
    }
   ],
   "source": [
    "import pickle as pk\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import optparse\n",
    "import logging\n",
    "\n",
    "import optparse\n",
    "import logging\n",
    "import os\n",
    "\n",
    "def get_arguments():\n",
    "    parser = optparse.OptionParser()\n",
    "    \n",
    "    parser.add_option(\"-d\", \"--data\", dest=\"dataset_prefix\", default=\"data/hansards\", help=\"Data file prefix (default=data)\")\n",
    "    parser.add_option(\"-e\", \"--english\", dest=\"english_suffix\", default=\"e\", help=\"English file suffix (default=e)\")\n",
    "    parser.add_option(\"-f\", \"--french\", dest=\"french_suffix\", default=\"f\", help=\"French file suffix (default=f)\")\n",
    "    parser.add_option(\"--eps\", dest=\"epsilon\", default=1e-3, type=\"float\", help=\"Error threshold for stopping the EM algorithm (default=1e-3)\")\n",
    "    parser.add_option(\"-n\", \"--num_sentences\", dest=\"sentence_count\", default=100000000000, type=\"int\", help=\"Number of sentences for training\")\n",
    "    parser.add_option(\"-l\", \"--lower\", action=\"store_true\", help=\"Lowercase the corpus (default=False)\")\n",
    "    parser.add_option(\"-g\", \"--debugging\", action=\"store_true\", help=\"Enable debugging (default=False)\")\n",
    "    parser.add_option(\"-s\", \"--save\", dest=\"model_save_path\", default=\"ibm1.pkl\", help=\"Path to save the model\")\n",
    "    parser.add_option(\"-r\", \"--save_r\", dest=\"reversed_model_save_path\", default=\"ibm1_r.pkl\", help=\"Path to save the reversed model\")\n",
    "    parser.add_option(\"-p\", \"--pretrained\", dest=\"model_load_path\", default=\"\", help=\"Path to load the model\")\n",
    "    parser.add_option(\"-t\", \"--pretrained_r\", dest=\"reversed_model_load_path\", default=\"\", help=\"Path to load the reversed model\")\n",
    "    parser.add_option(\"-m\", \"--max_iterations\", dest=\"max_iterations\", default=500, type=\"int\", help=\"Maximum training iterations\")\n",
    "    parser.add_option(\"-c\", \"--continue\", dest=\"current_iteration\", type=int, default=None, help=\"Resume from given iteration. Requires -p option.\")\n",
    "    \n",
    "    options, _ = parser.parse_args()\n",
    "\n",
    "    french_file = f\"{options.dataset_prefix}.{options.french_suffix}\"\n",
    "    english_file = f\"{options.dataset_prefix}.{options.english_suffix}\"\n",
    "\n",
    "    if options.current_iteration:\n",
    "        assert options.model_load_path, \"Using -c requires -p\"\n",
    "\n",
    "    for model_path in [options.model_save_path, options.reversed_model_save_path]:\n",
    "        if not (model_path == options.model_load_path or model_path == options.reversed_model_load_path):\n",
    "            base_name, extension = os.path.splitext(model_path)\n",
    "            new_base = f\"{base_name}_num{options.sentence_count}_eps{options.epsilon}\"\n",
    "            new_path = new_base + extension\n",
    "            directory, file_name = os.path.split(new_path)\n",
    "            new_directory = os.path.join(directory, new_base)\n",
    "\n",
    "            assert not os.path.exists(new_directory), f\"{new_directory} already exists.\"\n",
    "            os.makedirs(new_directory)\n",
    "\n",
    "            if model_path == options.model_save_path:\n",
    "                options.model_save_path = os.path.join(new_directory, file_name)\n",
    "            else:\n",
    "                options.reversed_model_save_path = os.path.join(new_directory, file_name)\n",
    "\n",
    "    return options, french_file, english_file\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, corpus, case_sensitive=True) -> None:\n",
    "        words_set = set()\n",
    "        for sent in corpus:\n",
    "            for word in sent:\n",
    "                if not case_sensitive:\n",
    "                    word = word.lower()\n",
    "                words_set.add(word)\n",
    "        self.i2w = list(words_set)\n",
    "        self.w2i = {word:index for index, word in enumerate(self.i2w)}\n",
    "        self.V = len(self.i2w)\n",
    "    \n",
    "    def encode(self, sent):\n",
    "\n",
    "        return [self.w2i[word] for word in sent]\n",
    "    \n",
    "    def decode(self, sent_ind):\n",
    "\n",
    "        return [self.i2w[id] for id in sent_ind]\n",
    "\n",
    "\n",
    "class IBMModel1:\n",
    "    def __init__(self, bitext, f_tokenizer, e_tokenizer, save_path, eps=1e-5, max_iters=500) -> None:\n",
    "        '''\n",
    "        For all given f, \\sum_e t(e|f) = 1\n",
    "\n",
    "        Suppose Vocabulary size of e and f are Ve and Vf.\n",
    "        Use a Vf*Ve matrix to represent t(e|f), where the rows are different f\n",
    "        and the columns are different e. Thus, each row sum to 1.\n",
    "        '''\n",
    "        bitext_tokenized = [[f_tokenizer.encode(sent[0]), e_tokenizer.encode(sent[1])] for sent in bitext]\n",
    "        self.bitext = bitext_tokenized\n",
    "        '''Integerized parallel corpus'''\n",
    "        self.f_tokenizer = f_tokenizer\n",
    "        self.e_tokenizer = e_tokenizer\n",
    "        self.Vf = f_tokenizer.V\n",
    "        self.Ve = e_tokenizer.V\n",
    "        self.t = np.ones((self.Vf, self.Ve)) / self.Ve # so that each entry is uniformly 1 / Ve (row normalized)\n",
    "        self.save_path = save_path\n",
    "\n",
    "        self.eps = eps\n",
    "        self.max_iters = max_iters\n",
    "        self.is_converged = False\n",
    "        self.diff = -1\n",
    "    \n",
    "    def save(self, iters):\n",
    "        base, ext = os.path.splitext(opts.save_path)\n",
    "        base = f\"{base}_iter{iters}\"\n",
    "        path = base + ext\n",
    "        pickle.dump(self, open(path, \"wb\"))\n",
    "\n",
    "    def em_train(self, iter_now=0):\n",
    "        iters = iter_now\n",
    "        while (iters < self.max_iters) and (not self.is_converged):\n",
    "            iters += 1\n",
    "            logging.info(f\"Iteration {iters}, diff={self.diff:.8f}\")\n",
    "            logging.debug(f\"t(the|la)={self.t_e_f('the', 'la')}\")\n",
    "            logging.debug(f\"t(of|la)={self.t_e_f('of', 'la')}\")\n",
    "\n",
    "            # initialize\n",
    "            count = np.zeros((self.Vf, self.Ve))\n",
    "            total = np.zeros(self.Vf)\n",
    "\n",
    "            for f_sent, e_sent in self.bitext:\n",
    "                # compute normalization\n",
    "                s_total = {}\n",
    "                for e in e_sent:\n",
    "                    s_total[e] = 0\n",
    "                    for f in f_sent:\n",
    "                        s_total[e] += self.t[f, e]\n",
    "                # collect counts\n",
    "                for e in e_sent:\n",
    "                    for f in f_sent:\n",
    "                        count[f, e] += self.t[f, e]/s_total[e]\n",
    "                        total[f] += self.t[f, e]/s_total[e]\n",
    "            \n",
    "            # estimate probabilities\n",
    "            self.diff = 0\n",
    "            for f in range(self.Vf):\n",
    "                for e in range(self.Ve):\n",
    "                    new_t = count[f, e] / total[f]\n",
    "                    el_diff = np.abs(new_t - self.t[f, e])\n",
    "                    self.diff = max(self.diff, el_diff)\n",
    "                    self.t[f, e] = new_t\n",
    "\n",
    "            # check convergence and save\n",
    "            if self.diff < self.eps:\n",
    "                self.is_converged = True\n",
    "            self.save(iters)\n",
    "\n",
    "        if self.is_converged:\n",
    "            logging.info(f\"Model converged after {iters} iteration under error {self.eps}!\")\n",
    "        else:\n",
    "            logging.info(f\"Training stopped after reaching max number of iterations {self.max_iters}\")\n",
    "\n",
    "    def t_e_f(self, e, f):\n",
    "        '''\n",
    "        Computes t(e|f), where e & f are both string words\n",
    "        \n",
    "        If either e or f is not in the vocabulary, return -1\n",
    "        '''\n",
    "        try:\n",
    "            ei = self.e_tokenizer.w2i[e]\n",
    "            fi = self.f_tokenizer.w2i[f]\n",
    "        except KeyError:\n",
    "            return -1\n",
    "        return self.t[fi, ei]\n",
    "\n",
    "def align(model, model_r, bitext):\n",
    "    \n",
    "    for (f, e) in bitext:\n",
    "        alignment = []\n",
    "        alignment_r = []\n",
    "        for (i, f_i) in enumerate(f): \n",
    "            max_prob = 0\n",
    "            best_j = 0\n",
    "            for (j, e_j) in enumerate(e):\n",
    "                tmp_prob = model.t_e_f(e_j, f_i)\n",
    "                if tmp_prob > max_prob:\n",
    "                    best_j = j\n",
    "                    max_prob = tmp_prob\n",
    "            if(max_prob > 0):\n",
    "                alignment.append([i,best_j])\n",
    "\n",
    "        for (j, e_j) in enumerate(e):\n",
    "            max_prob = 0\n",
    "            best_i = 0\n",
    "            for (i, f_i) in enumerate(f):\n",
    "                tmp_prob = model_r.t_e_f(f_i, e_j)\n",
    "                if tmp_prob > max_prob:\n",
    "                    best_i = i\n",
    "                    max_prob = tmp_prob\n",
    "            if(max_prob > 0):\n",
    "                alignment_r.append([best_i,j])\n",
    "    \n",
    "        for i in range(len(f)):\n",
    "            for j in range(len(e)):\n",
    "                if [i, j] in alignment and [i, j] in alignment_r:\n",
    "                    sys.stdout.write(\"%i-%i \" % (i, j))\n",
    "\n",
    "        sys.stdout.write(\"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    opts, f_data, e_data = parse_args()\n",
    "    loglvl = logging.DEBUG if opts.debugging else logging.INFO\n",
    "    logging.basicConfig(level=loglvl)\n",
    "    bitext = [[sentence.strip().split() for sentence in pair] for pair in zip(open(f_data), open(e_data))][:opts.num_sents]\n",
    "    bitext_r = [[sentence.strip().split() for sentence in pair] for pair in zip(open(e_data), open(f_data))][:opts.num_sents]\n",
    "    if opts.lower:\n",
    "        bitext = [[[w.lower() for w in fs],[w.lower() for w in es]] for fs, es in bitext]\n",
    "    f_text = [sent[0] for sent in bitext]\n",
    "    e_text = [sent[1] for sent in bitext]\n",
    "    f_tokenizer = Tokenizer(f_text)\n",
    "    e_tokenizer = Tokenizer(e_text)\n",
    "    \n",
    "    f_text_r = [sent[0] for sent in bitext_r]\n",
    "    e_text_r = [sent[1] for sent in bitext_r]\n",
    "    f_tokenizer_r = Tokenizer(e_text)\n",
    "    e_tokenizer_r = Tokenizer(f_text)\n",
    "\n",
    "    # bitext_tokenized = []\n",
    "    # for sent in bitext:\n",
    "    #     print(sent)\n",
    "    #     bitext_tokenized.append([f_tokenizer.encode(sent[0]), e_tokenizer.encode(sent[1])])\n",
    "\n",
    "    logging.info(f\"Vocabulary size - foreign:{f_tokenizer.V}, english:{e_tokenizer.V}\")\n",
    "    if opts.load_path:\n",
    "        ibm = pickle.load(open(opts.load_path, \"rb\"))\n",
    "        if opts.iter_now is not None:\n",
    "            ibm.em_train(opts.iter_now)\n",
    "    else:\n",
    "        ibm = IBMModel1(bitext, f_tokenizer, e_tokenizer, opts.save_path, eps=opts.eps, max_iters=opts.max_iters)\n",
    "        ibm.em_train()\n",
    "        \n",
    "    if opts.load_path_r:\n",
    "        ibm_r = pickle.load(open(opts.load_path_r, \"rb\"))\n",
    "        if opts.iter_now is not None:\n",
    "            ibm_r.em_train(opts.iter_now)\n",
    "    else:\n",
    "        ibm_r = IBMModel1(bitext_r, f_tokenizer_r, e_tokenizer_r, opts.save_path_r, eps=opts.eps, max_iters=opts.max_iters)\n",
    "        ibm_r.em_train()\n",
    "\n",
    "    align(ibm, ibm_r, bitext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a66e1ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
