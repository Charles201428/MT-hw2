{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfa8640e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import optparse\n",
    "import logging\n",
    "\n",
    "def parse():\n",
    "    optparser = optparse.OptionParser()\n",
    "    optparser.add_option(\"-d\", \"--data\", dest=\"train\", default=\"data/hansards\", help=\"Data filename prefix (default=data)\")\n",
    "    optparser.add_option(\"-e\", \"--english\", dest=\"english\", default=\"e\", help=\"Suffix of English filename (default=e)\")\n",
    "    optparser.add_option(\"-f\", \"--french\", dest=\"french\", default=\"f\", help=\"Suffix of French filename (default=f)\")\n",
    "    optparser.add_option(\"--eps\", dest=\"eps\", default=1e-3, type=\"float\", help=\"Default error bound for stopping the EM algorithm (default=1e-3)\")\n",
    "    optparser.add_option(\"-n\", \"--num_sentences\", dest=\"num_sents\", default=100000000000, type=\"int\", help=\"Number of sentences to use for training and alignment\")\n",
    "    optparser.add_option(\"-l\", \"--lower\", action=\"store_true\", dest=\"lower\", help=\"Convert corpus to lower case (defualt=False)\")\n",
    "    optparser.add_option(\"-g\", \"--debugging\", action=\"store_true\", dest=\"debugging\", help=\"Turn on debugging mode (defualt=False)\")\n",
    "    optparser.add_option(\"-s\", \"--save\", dest=\"save_path\", default=\"ibm1.pkl\", help=\"Model save path\")\n",
    "    optparser.add_option(\"-p\", \"--pretrained\", dest=\"load_path\", default=\"\", help=\"Model save path\")\n",
    "    optparser.add_option(\"-m\", \"--max_iterations\", dest=\"max_iters\", default=500, type=\"int\", help=\"Max number of iterations to train\")\n",
    "    optparser.add_option(\"-c\", \"--continue\", dest=\"iter_now\", type=int, default=None, help=\"Continue with iteration number provided by this option. Need to have -p option turned on\")\n",
    "    (opts, _) = optparser.parse_args()\n",
    "\n",
    "    french = f\"{opts.train}.{opts.french}\"\n",
    "    english = f\"{opts.train}.{opts.english}\"\n",
    "\n",
    "    test(opts)\n",
    "\n",
    "    return opts, french, english\n",
    "\n",
    "\n",
    "\n",
    "def test (a) -> None:\n",
    "    if a.iter_now is not None:\n",
    "        assert a.load_path\n",
    "        \n",
    "    if not a.load_path:\n",
    "        c, d = os.path.splitext(a.save_path)\n",
    "        c = f\"{c}_num{a.num_sents}_eps{a.eps}\"\n",
    "        e = c + d\n",
    "        f, g = os.path.split(e)\n",
    "        f = os.path.join(f, c)\n",
    "        assert not os.path.exists(f), f\"{f} occupied.\"\n",
    "        os.makedirs(f)\n",
    "        a.save_path = os.path.join(f, g)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, corpus, case_sensitive=True) -> None:\n",
    "        words_set = set()\n",
    "        for inpu in corpus:\n",
    "            for choice in inpu:\n",
    "                if case_sensitive:\n",
    "                    words_set.add(choice)\n",
    "                else:\n",
    "                    choice = choice.lower()\n",
    "                    words_set.add(choice)\n",
    "        self.toWord = list(words_set)\n",
    "        self.toIndex = {choice:index for index, choice in enumerate(self.toWord)}\n",
    "        self.V = len(self.toWord)\n",
    "    \n",
    "    def encode(self, inpu):\n",
    "\n",
    "        encoderesult = [self.toIndex[word] for word in inpu]\n",
    "\n",
    "        return encoderesult\n",
    "    \n",
    "    def decode(self, inpuIndex):\n",
    "\n",
    "        decoderesult = [self.toWord[id] for id in inpuIndex]   \n",
    "        return decoderesult\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class IBMModel1:\n",
    "    def __init__(self, bitext, f_tokenizer, e_tokenizer, save_path, eps=1e-5, max_iters=500) -> None:\n",
    "        '''\n",
    "        For all given f, \\sum_e t(e|f) = 1\n",
    "\n",
    "        Suppose Vocabulary size of e and f are Ve and Vf.\n",
    "        Use a Vf*Ve matrix to represent t(e|f), where the rows are different f\n",
    "        and the columns are different e. Thus, each row sum to 1.\n",
    "        '''\n",
    "        bitext_tokenized = [[f_tokenizer.encode(inpu[0]), e_tokenizer.encode(inpu[1])] for inpu in bitext]\n",
    "        self.bitext = bitext_tokenized\n",
    "        '''Integerized parallel corpus'''\n",
    "        self.f_tokenizer = f_tokenizer\n",
    "        self.e_tokenizer = e_tokenizer\n",
    "        self.Vf = f_tokenizer.V\n",
    "        self.Ve = e_tokenizer.V\n",
    "        self.t = np.ones((self.Vf, self.Ve)) / self.Ve # so that each entry is uniformly 1 / Ve (row normalized)\n",
    "        self.save_path = save_path\n",
    "\n",
    "        self.eps = eps\n",
    "        self.max_iters = max_iters\n",
    "        self.is_converged = False\n",
    "        self.diff = -1\n",
    "    \n",
    "    def save(self, iters):\n",
    "        base, ext = os.path.splitext(opts.save_path)\n",
    "        base = f\"{base}_iter{iters}\"\n",
    "        path = base + ext\n",
    "        pickle.dump(self, open(path, \"wb\"))\n",
    "\n",
    "    def em_train(self, iter_now=0):\n",
    "        iters = iter_now\n",
    "        while (iters < self.max_iters) and (not self.is_converged):\n",
    "            iters += 1\n",
    "            logging.info(f\"Iteration {iters}, diff={self.diff:.8f}\")\n",
    "            logging.debug(f\"t(the|la)={self.t_e_f('the', 'la')}\")\n",
    "            logging.debug(f\"t(of|la)={self.t_e_f('of', 'la')}\")\n",
    "\n",
    "            # initialize\n",
    "            count = np.zeros((self.Vf, self.Ve))\n",
    "            total = np.zeros(self.Vf)\n",
    "\n",
    "            for f_sent, e_sent in self.bitext:\n",
    "                # compute normalization\n",
    "                s_total = {}\n",
    "                for e in e_sent:\n",
    "                    s_total[e] = 0\n",
    "                    for f in f_sent:\n",
    "                        s_total[e] += self.t[f, e]\n",
    "                # collect counts\n",
    "                for e in e_sent:\n",
    "                    for f in f_sent:\n",
    "                        count[f, e] += self.t[f, e]/s_total[e]\n",
    "                        total[f] += self.t[f, e]/s_total[e]\n",
    "            \n",
    "            # estimate probabilities\n",
    "            self.diff = 0\n",
    "            for f in range(self.Vf):\n",
    "                for e in range(self.Ve):\n",
    "                    new_t = count[f, e] / total[f]\n",
    "                    el_diff = np.abs(new_t - self.t[f, e])\n",
    "                    self.diff = max(self.diff, el_diff)\n",
    "                    self.t[f, e] = new_t\n",
    "\n",
    "            # check convergence and save\n",
    "            if self.diff < self.eps:\n",
    "                self.is_converged = True\n",
    "            self.save(iters)\n",
    "\n",
    "        if self.is_converged:\n",
    "            logging.info(f\"Model converged after {iters} iteration under error {self.eps}!\")\n",
    "        else:\n",
    "            logging.info(f\"Training stopped after reaching max number of iterations {self.max_iters}\")\n",
    "\n",
    "    def t_e_f(self, e, f):\n",
    "        '''\n",
    "        Computes t(e|f), where e & f are both string words\n",
    "        \n",
    "        If either e or f is not in the vocabulary, return -1\n",
    "        '''\n",
    "        try:\n",
    "            ei = self.e_tokenizer.toIndex[e]\n",
    "            fi = self.f_tokenizer.toIndex[f]\n",
    "        except KeyError:\n",
    "            return -1\n",
    "        return self.t[fi, ei]\n",
    "\n",
    "def align(model, bitext):\n",
    "    for (f, e) in bitext:\n",
    "        for (i, f_i) in enumerate(f): \n",
    "            max_prob = 0\n",
    "            best_j = 0\n",
    "            for (j, e_j) in enumerate(e):\n",
    "                if model.t_e_f(e_j, f_i) > max_prob:\n",
    "                    best_j = j\n",
    "                    max_prob = model.t_e_f(e_j, f_i)\n",
    "            if(max_prob > 0):\n",
    "                sys.stdout.write(\"%i-%i \" % (i,best_j))\n",
    "        sys.stdout.write(\"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    opts, f_data, e_data = parse()\n",
    "    loglvl = logging.DEBUG if opts.debugging else logging.INFO\n",
    "    logging.basicConfig(level=loglvl)\n",
    "    bitext = [[sentence.strip().split() for sentence in pair] for pair in zip(open(f_data), open(e_data))][:opts.num_sents]\n",
    "    if opts.lower:\n",
    "        bitext = [[[w.lower() for w in fs],[w.lower() for w in es]] for fs, es in bitext]\n",
    "    f_text = [inpu[0] for inpu in bitext]\n",
    "    e_text = [inpu[1] for inpu in bitext]\n",
    "    f_tokenizer = Tokenizer(f_text)\n",
    "    e_tokenizer = Tokenizer(e_text)\n",
    "\n",
    "    logging.info(f\"Vocabulary size - foreign:{f_tokenizer.V}, english:{e_tokenizer.V}\")\n",
    "    if opts.load_path:\n",
    "        ibm = pickle.load(open(opts.load_path, \"rb\"))\n",
    "        if opts.iter_now is not None:\n",
    "            ibm.em_train(opts.iter_now)\n",
    "    else:\n",
    "        ibm = IBMModel1(bitext, f_tokenizer, e_tokenizer, opts.save_path, eps=opts.eps, max_iters=opts.max_iters)\n",
    "        ibm.em_train()\n",
    "    align(ibm, bitext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef003ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import optparse\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "optparser = optparse.OptionParser()\n",
    "optparser.add_option(\"-d\", \"--data\", dest=\"train\", default=\"data/hansards\", help=\"Data filename prefix (default=data)\")\n",
    "optparser.add_option(\"-e\", \"--english\", dest=\"english\", default=\"e\", help=\"Suffix of English filename (default=e)\")\n",
    "optparser.add_option(\"-f\", \"--french\", dest=\"french\", default=\"f\", help=\"Suffix of French filename (default=f)\")\n",
    "optparser.add_option(\"-t\", \"--threshold\", dest=\"threshold\", default=0.5, type=\"float\", help=\"Threshold for aligning with Dice's coefficient (default=0.5)\")\n",
    "optparser.add_option(\"-n\", \"--num_sentences\", dest=\"num_sents\", default=100000000000, type=\"int\", help=\"Number of sentences to use for training and alignment\")\n",
    "(opts, _) = optparser.parse_args()\n",
    "f_data = \"%s.%s\" % (opts.train, opts.french)\n",
    "e_data = \"%s.%s\" % (opts.train, opts.english)\n",
    "\n",
    "sys.stderr.write(\"Training with Dice's coefficient...\")\n",
    "bitext = [[sentence.strip().split() for sentence in pair] for pair in zip(open(f_data), open(e_data))][:opts.num_sents]\n",
    "f_count = defaultdict(int)\n",
    "e_count = defaultdict(int)\n",
    "fe_count = defaultdict(int)\n",
    "for (n, (f, e)) in enumerate(bitext):\n",
    "  for f_i in set(f):\n",
    "    f_count[f_i] += 1\n",
    "    for e_j in set(e):\n",
    "      fe_count[(f_i,e_j)] += 1\n",
    "  for e_j in set(e):\n",
    "    e_count[e_j] += 1\n",
    "  if n % 500 == 0:\n",
    "    sys.stderr.write(\".\")\n",
    "\n",
    "dice = defaultdict(int)\n",
    "for (k, (f_i, e_j)) in enumerate(fe_count.keys()):\n",
    "  dice[(f_i,e_j)] = 2.0 * fe_count[(f_i, e_j)] / (f_count[f_i] + e_count[e_j])\n",
    "  if k % 5000 == 0:\n",
    "    sys.stderr.write(\".\")\n",
    "sys.stderr.write(\"\\n\")\n",
    "\n",
    "for (f, e) in bitext:\n",
    "  for (i, f_i) in enumerate(f): \n",
    "    for (j, e_j) in enumerate(e):\n",
    "      if dice[(f_i,e_j)] >= opts.threshold:\n",
    "        sys.stdout.write(\"%i-%i \" % (i,j))\n",
    "  sys.stdout.write(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
