{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfa8640e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import optparse\n",
    "import logging\n",
    "\n",
    "def parse():\n",
    "    optparser = optparse.OptionParser()\n",
    "    optparser.add_option(\"-d\", \"--data\", dest=\"train\", default=\"data/hansards\", help=\"Data filename prefix (default=data)\")\n",
    "    optparser.add_option(\"-e\", \"--english\", dest=\"english\", default=\"e\", help=\"Suffix of English filename (default=e)\")\n",
    "    optparser.add_option(\"-f\", \"--french\", dest=\"french\", default=\"f\", help=\"Suffix of French filename (default=f)\")\n",
    "    optparser.add_option(\"--eps\", dest=\"eps\", default=1e-3, type=\"float\", help=\"Default error bound for stopping the EM algorithm (default=1e-3)\")\n",
    "    optparser.add_option(\"-n\", \"--num_sentences\", dest=\"num_sents\", default=100000000000, type=\"int\", help=\"Number of sentences to use for training and alignment\")\n",
    "    optparser.add_option(\"-l\", \"--lower\", action=\"store_true\", dest=\"lower\", help=\"Convert corpus to lower case (defualt=False)\")\n",
    "    optparser.add_option(\"-g\", \"--debugging\", action=\"store_true\", dest=\"debugging\", help=\"Turn on debugging mode (defualt=False)\")\n",
    "    optparser.add_option(\"-s\", \"--save\", dest=\"save_path\", default=\"ibm1.pkl\", help=\"Model save path\")\n",
    "    optparser.add_option(\"-p\", \"--pretrained\", dest=\"load_path\", default=\"\", help=\"Model save path\")\n",
    "    optparser.add_option(\"-m\", \"--max_iterations\", dest=\"max_iters\", default=500, type=\"int\", help=\"Max number of iterations to train\")\n",
    "    optparser.add_option(\"-c\", \"--continue\", dest=\"iter_now\", type=int, default=None, help=\"Continue with iteration number provided by this option. Need to have -p option turned on\")\n",
    "    (opts, _) = optparser.parse_args()\n",
    "\n",
    "    french = f\"{opts.train}.{opts.french}\"\n",
    "    english = f\"{opts.train}.{opts.english}\"\n",
    "\n",
    "    test(opts)\n",
    "\n",
    "    return opts, french, english\n",
    "\n",
    "\n",
    "\n",
    "def test (a) -> None:\n",
    "    if a.iter_now is not None:\n",
    "        assert a.load_path\n",
    "        \n",
    "    if not a.load_path:\n",
    "        c, d = os.path.splitext(a.save_path)\n",
    "        c = f\"{c}_num{a.num_sents}_eps{a.eps}\"\n",
    "        e = c + d\n",
    "        f, g = os.path.split(e)\n",
    "        f = os.path.join(f, c)\n",
    "        assert not os.path.exists(f), f\"{f} occupied.\"\n",
    "        os.makedirs(f)\n",
    "        a.save_path = os.path.join(f, g)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, corpus, case_sensitive=True) -> None:\n",
    "        words_set = set()\n",
    "        for inpu in corpus:\n",
    "            for choice in inpu:\n",
    "                if case_sensitive:\n",
    "                    words_set.add(choice)\n",
    "                else:\n",
    "                    choice = choice.lower()\n",
    "                    words_set.add(choice)\n",
    "        self.toWord = list(words_set)\n",
    "        self.toIndex = {choice:index for index, choice in enumerate(self.toWord)}\n",
    "        self.V = len(self.toWord)\n",
    "    \n",
    "    def encode(self, inpu):\n",
    "\n",
    "        encoderesult = [self.toIndex[word] for word in inpu]\n",
    "\n",
    "        return encoderesult\n",
    "    \n",
    "    def decode(self, inpuIndex):\n",
    "\n",
    "        decoderesult = [self.toWord[id] for id in inpuIndex]   \n",
    "        return decoderesult\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class IBMModel1:\n",
    "    def __init__(self, bilingual_text, source_tokenizer, target_tokenizer, save_directory, tolerance=1e-5, max_iterations=50):\n",
    "        tokenized_text = [[source_tokenizer.encode(pair[0]), target_tokenizer.encode(pair[1])] for pair in bilingual_text]\n",
    "        self.bilingual_text = tokenized_text\n",
    "        self.source_tokenizer = source_tokenizer\n",
    "        self.target_tokenizer = target_tokenizer\n",
    "        self.source_vocab_size = source_tokenizer.V\n",
    "        self.target_vocab_size = target_tokenizer.V\n",
    "        self.translation_prob = np.ones((self.source_vocab_size, self.target_vocab_size)) / self.target_vocab_size\n",
    "        self.save_directory = save_directory\n",
    "\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iterations = max_iterations\n",
    "        self.has_converged = False\n",
    "        self.change = -1\n",
    "    \n",
    "    def save(self, iteration_count):\n",
    "        base_name, file_extension = os.path.splitext(opts.save_directory)\n",
    "        base_name = f\"{base_name}_iter{iteration_count}\"\n",
    "        path = base_name + file_extension\n",
    "        pk.dump(self, open(path, \"wb\"))\n",
    "\n",
    "    def train(self, current_iteration=0):\n",
    "        iteration = current_iteration\n",
    "        while (iteration < self.max_iterations) and (not self.has_converged):\n",
    "            iteration += 1\n",
    "            logging.info(f\"Iteration {iteration}, change={self.change:.8f}\")\n",
    "            logging.debug(f\"translation_prob(the|la)={self.get_prob('the', 'la')}\")\n",
    "            logging.debug(f\"translation_prob(of|la)={self.get_prob('of', 'la')}\")\n",
    "\n",
    "            count_matrix = np.zeros((self.source_vocab_size, self.target_vocab_size))\n",
    "            total_vector = np.zeros(self.source_vocab_size)\n",
    "\n",
    "            for source_sentence, target_sentence in self.bilingual_text:\n",
    "                sentence_total = {}\n",
    "                for target_word in target_sentence:\n",
    "                    sentence_total[target_word] = 0\n",
    "                    for source_word in source_sentence:\n",
    "                        sentence_total[target_word] += self.translation_prob[source_word, target_word]\n",
    "\n",
    "                for target_word in target_sentence:\n",
    "                    for source_word in source_sentence:\n",
    "                        count_matrix[source_word, target_word] += self.translation_prob[source_word, target_word]/sentence_total[target_word]\n",
    "                        total_vector[source_word] += self.translation_prob[source_word, target_word]/sentence_total[target_word]\n",
    "            \n",
    "            self.change = 0\n",
    "            for source_word_idx in range(self.source_vocab_size):\n",
    "                for target_word_idx in range(self.target_vocab_size):\n",
    "                    updated_prob = count_matrix[source_word_idx, target_word_idx] / total_vector[source_word_idx]\n",
    "                    element_change = np.abs(updated_prob - self.translation_prob[source_word_idx, target_word_idx])\n",
    "                    self.change = max(self.change, element_change)\n",
    "                    self.translation_prob[source_word_idx, target_word_idx] = updated_prob\n",
    "\n",
    "            if self.change < self.tolerance:\n",
    "                self.has_converged = True\n",
    "            self.save(iteration)\n",
    "\n",
    "        if self.has_converged:\n",
    "            logging.info(f\"Model converged after {iteration} iteration under error {self.tolerance}!\")\n",
    "        else:\n",
    "            logging.info(f\"Training stopped after reaching max number of iterations {self.max_iterations}\")\n",
    "\n",
    "    def get_prob(self, target_word, source_word):\n",
    "        try:\n",
    "            target_index = self.target_tokenizer.toIndex[target_word]\n",
    "            source_index = self.source_tokenizer.toIndex[source_word]\n",
    "        except KeyError:\n",
    "            return -1\n",
    "        return self.translation_prob[source_index, target_index]\n",
    "\n",
    "\n",
    "def align_words(translation_model, bilingual_text):\n",
    "    for (source_sentence, target_sentence) in bilingual_text:\n",
    "        for (source_index, source_word) in enumerate(source_sentence): \n",
    "            highest_probability = 0\n",
    "            best_target_index = 0\n",
    "            for (target_index, target_word) in enumerate(target_sentence):\n",
    "                if translation_model.t_e_f(target_word, source_word) > highest_probability:\n",
    "                    best_target_index = target_index\n",
    "                    highest_probability = translation_model.t_e_f(target_word, source_word)\n",
    "            if(highest_probability > 0):\n",
    "                sys.stdout.write(\"%i-%i \" % (source_index, best_target_index))\n",
    "        sys.stdout.write(\"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    o, f, e = parse()\n",
    "    b = [[sentence.strip().split() for sentence in pair] for pair in zip(open(f_data), open(e_data))][:opts.num_sents]\n",
    "    if opts.lower:\n",
    "        bitext = [[[w.lower() for w in fs],[w.lower() for w in es]] for fs, es in bitext]\n",
    "    f_text = [inpu[0] for inpu in bitext]\n",
    "    e_text = [inpu[1] for inpu in bitext]\n",
    "    f_tokenizer = Tokenizer(f_text)\n",
    "    e_tokenizer = Tokenizer(e_text)\n",
    "\n",
    "    logging.info(f\"Vocabulary size - foreign:{f_tokenizer.V}, english:{e_tokenizer.V}\")\n",
    "    if opts.load_path:\n",
    "        ibm = pk.load(open(opts.load_path, \"rb\"))\n",
    "        if opts.iter_now is not None:\n",
    "            ibm.train(opts.iter_now)\n",
    "    else:\n",
    "        ibm = IBMModel1(bitext, f_tokenizer, e_tokenizer, opts.save_path, tolerance=opts.eps, max_iterations=opts.max_iters)\n",
    "        ibm.train()\n",
    "    align(ibm, bitext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cb4abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import optparse\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "optparser = optparse.OptionParser()\n",
    "optparser.add_option(\"-d\", \"--data\", dest=\"train\", default=\"data/hansards\", help=\"Data filename prefix (default=data)\")\n",
    "optparser.add_option(\"-e\", \"--english\", dest=\"english\", default=\"e\", help=\"Suffix of English filename (default=e)\")\n",
    "optparser.add_option(\"-f\", \"--french\", dest=\"french\", default=\"f\", help=\"Suffix of French filename (default=f)\")\n",
    "optparser.add_option(\"-t\", \"--threshold\", dest=\"threshold\", default=0.5, type=\"float\", help=\"Threshold for aligning with Dice's coefficient (default=0.5)\")\n",
    "optparser.add_option(\"-n\", \"--num_sentences\", dest=\"num_sents\", default=100000000000, type=\"int\", help=\"Number of sentences to use for training and alignment\")\n",
    "(opts, _) = optparser.parse_args()\n",
    "f_data = \"%s.%s\" % (opts.train, opts.french)\n",
    "e_data = \"%s.%s\" % (opts.train, opts.english)\n",
    "\n",
    "sys.stderr.write(\"Training with Dice's coefficient...\")\n",
    "bitext = [[sentence.strip().split() for sentence in pair] for pair in zip(open(f_data), open(e_data))][:opts.num_sents]\n",
    "f_count = defaultdict(int)\n",
    "e_count = defaultdict(int)\n",
    "fe_count = defaultdict(int)\n",
    "for (n, (f, e)) in enumerate(bitext):\n",
    "  for f_i in set(f):\n",
    "    f_count[f_i] += 1\n",
    "    for e_j in set(e):\n",
    "      fe_count[(f_i,e_j)] += 1\n",
    "  for e_j in set(e):\n",
    "    e_count[e_j] += 1\n",
    "  if n % 500 == 0:\n",
    "    sys.stderr.write(\".\")\n",
    "\n",
    "dice = defaultdict(int)\n",
    "for (k, (f_i, e_j)) in enumerate(fe_count.keys()):\n",
    "  dice[(f_i,e_j)] = 2.0 * fe_count[(f_i, e_j)] / (f_count[f_i] + e_count[e_j])\n",
    "  if k % 5000 == 0:\n",
    "    sys.stderr.write(\".\")\n",
    "sys.stderr.write(\"\\n\")\n",
    "\n",
    "for (f, e) in bitext:\n",
    "  for (i, f_i) in enumerate(f): \n",
    "    for (j, e_j) in enumerate(e):\n",
    "      if dice[(f_i,e_j)] >= opts.threshold:\n",
    "        sys.stdout.write(\"%i-%i \" % (i,j))\n",
    "  sys.stdout.write(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
