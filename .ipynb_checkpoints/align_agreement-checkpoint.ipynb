{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88e449e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'data/hansards.C:\\\\Users\\\\charl\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-ffeb52ad-4d08-443c-9473-1a32d7efb05a.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27224/1844393840.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[0mloglvl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEBUG\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebugging\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasicConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloglvl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m     \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_sents\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[0mbr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_sents\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[0mself_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'data/hansards.C:\\\\Users\\\\charl\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-ffeb52ad-4d08-443c-9473-1a32d7efb05a.json'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle as pk\n",
    "import sys\n",
    "import os\n",
    "import optparse\n",
    "import logging\n",
    "\n",
    "def parse():\n",
    "    optparser = optparse.OptionParser()\n",
    "    optparser.add_option(\"-d\", \"--data\", dest=\"train\", default=\"data/hansards\", help=\"Data filename prefix (default=data)\")\n",
    "    optparser.add_option(\"-e\", \"--english\", dest=\"english\", default=\"e\", help=\"Suffix of English filename (default=e)\")\n",
    "    optparser.add_option(\"-f\", \"--french\", dest=\"french\", default=\"f\", help=\"Suffix of French filename (default=f)\")\n",
    "    optparser.add_option(\"--eps\", dest=\"eps\", default=1e-3, type=\"float\", help=\"Default error bound for stopping the EM algorithm (default=1e-3)\")\n",
    "    optparser.add_option(\"-n\", \"--num_sentences\", dest=\"num_sents\", default=100000000000, type=\"int\", help=\"Number of sentences to use for training and alignment\")\n",
    "    optparser.add_option(\"-l\", \"--lower\", action=\"store_true\", dest=\"lower\", help=\"Convert corpus to lower case (defualt=False)\")\n",
    "    optparser.add_option(\"-g\", \"--debugging\", action=\"store_true\", dest=\"debugging\", help=\"Turn on debugging mode (defualt=False)\")\n",
    "    optparser.add_option(\"-s\", \"--save\", dest=\"save_path\", default=\"ibm1.pkl\", help=\"Model save path\")\n",
    "    optparser.add_option(\"-r\", \"--save_r\", dest=\"save_path_r\", default=\"ibm1_r.pkl\", help=\"Reversed model save path\")\n",
    "    optparser.add_option(\"-p\", \"--pretrained\", dest=\"load_path\", default=\"\", help=\"Model save path\")\n",
    "    optparser.add_option(\"-t\", \"--pretrained_r\", dest=\"load_path_r\", default=\"\", help=\"Reversed model save path\")\n",
    "    optparser.add_option(\"-m\", \"--max_iterations\", dest=\"max_iters\", default=500, type=\"int\", help=\"Max number of iterations to train\")\n",
    "    optparser.add_option(\"-c\", \"--continue\", dest=\"iter_now\", type=int, default=None, help=\"Continue with iteration number provided by this option. Need to have -p option turned on\")\n",
    "    (opts, _) = optparser.parse_args()\n",
    "\n",
    "    f = f\"{opts.train}.{opts.french}\"\n",
    "    e = f\"{opts.train}.{opts.english}\"\n",
    "\n",
    "    test(opts)\n",
    "    return opts, f, e\n",
    "\n",
    "\n",
    "def test (a) -> None:\n",
    "    if a.iter_now is not None:\n",
    "        assert a.load_path\n",
    "        \n",
    "    if not a.load_path:\n",
    "        c, d = os.path.splitext(a.save_path)\n",
    "        c = f\"{c}_num{a.num_sents}_eps{a.eps}\"\n",
    "        e = c + d\n",
    "        f, g = os.path.split(e)\n",
    "        f = os.path.join(f, c)\n",
    "        assert not os.path.exists(f), f\"{f} occupied.\"\n",
    "        os.makedirs(f)\n",
    "        a.save_path = os.path.join(f, g)\n",
    "        \n",
    "    if not a.load_path_r:\n",
    "        c, d = os.path.splitext(a.save_path_r)\n",
    "        c = f\"{c}_num{a.num_sents}_eps{a.eps}\"\n",
    "        e = c + d\n",
    "        f, g = os.path.split(e)\n",
    "        f = os.path.join(f, c)\n",
    "        assert not os.path.exists(f), f\"{f} occupied.\"\n",
    "        os.makedirs(f)\n",
    "        a.save_path = os.path.join(f, g)\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, corpus, case_sensitive=True) -> None:\n",
    "        words_set = set()\n",
    "        for inpu in corpus:\n",
    "            for choice in inpu:\n",
    "                if case_sensitive:\n",
    "                    words_set.add(choice)\n",
    "                else:\n",
    "                    choice = choice.lower()\n",
    "                    words_set.add(choice)\n",
    "        self.toWord = list(words_set)\n",
    "        self.toIndex = {choice:index for index, choice in enumerate(self.toWord)}\n",
    "        self.V = len(self.toWord)\n",
    "    \n",
    "    def encode(self, inpu):\n",
    "\n",
    "        encoderesult = [self.toIndex[word] for word in inpu]\n",
    "\n",
    "        return encoderesult\n",
    "    \n",
    "    def decode(self, inpuIndex):\n",
    "\n",
    "        decoderesult = [self.toWord[id] for id in inpuIndex]   \n",
    "        return decoderesult\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class IBMModel1:\n",
    "    def __init__(self, bilingual_text, source_tokenizer, target_tokenizer, save_directory, tolerance=1e-5, max_iterations=50):\n",
    "        tokenized_text = [[source_tokenizer.encode(pair[0]), target_tokenizer.encode(pair[1])] for pair in bilingual_text]\n",
    "        self.bilingual_text = tokenized_text\n",
    "        self.source_tokenizer = source_tokenizer\n",
    "        self.target_tokenizer = target_tokenizer\n",
    "        self.source_vocab_size = source_tokenizer.V\n",
    "        self.target_vocab_size = target_tokenizer.V\n",
    "        self.translation_prob = np.ones((self.source_vocab_size, self.target_vocab_size)) / self.target_vocab_size\n",
    "        self.save_directory = save_directory\n",
    "\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iterations = max_iterations\n",
    "        self.has_converged = False\n",
    "        self.change = -1\n",
    "    \n",
    "    def save(self, iteration_count):\n",
    "        base_name, file_extension = os.path.splitext(o.save_path)\n",
    "        base_name = f\"{base_name}_iter{iteration_count}\"\n",
    "        path = base_name + file_extension\n",
    "        pk.dump(self, open(path, \"wb\"))\n",
    "\n",
    "    def train(self, current_iteration=0):\n",
    "        iteration = current_iteration\n",
    "        while (iteration < self.max_iterations) and (not self.has_converged):\n",
    "            iteration += 1\n",
    "            logging.info(f\"Iteration {iteration}, change={self.change:.8f}\")\n",
    "            logging.debug(f\"translation_prob(the|la)={self.get_prob('the', 'la')}\")\n",
    "            logging.debug(f\"translation_prob(of|la)={self.get_prob('of', 'la')}\")\n",
    "\n",
    "            count_matrix = np.zeros((self.source_vocab_size, self.target_vocab_size))\n",
    "            total_vector = np.zeros(self.source_vocab_size)\n",
    "\n",
    "            for source_sentence, target_sentence in self.bilingual_text:\n",
    "                sentence_total = {}\n",
    "                for target_word in target_sentence:\n",
    "                    sentence_total[target_word] = 0\n",
    "                    for source_word in source_sentence:\n",
    "                        sentence_total[target_word] += self.translation_prob[source_word, target_word]\n",
    "\n",
    "                for target_word in target_sentence:\n",
    "                    for source_word in source_sentence:\n",
    "                        count_matrix[source_word, target_word] += self.translation_prob[source_word, target_word]/sentence_total[target_word]\n",
    "                        total_vector[source_word] += self.translation_prob[source_word, target_word]/sentence_total[target_word]\n",
    "            \n",
    "            self.change = 0\n",
    "            for source_word_idx in range(self.source_vocab_size):\n",
    "                for target_word_idx in range(self.target_vocab_size):\n",
    "                    updated_prob = count_matrix[source_word_idx, target_word_idx] / total_vector[source_word_idx]\n",
    "                    element_change = np.abs(updated_prob - self.translation_prob[source_word_idx, target_word_idx])\n",
    "                    self.change = max(self.change, element_change)\n",
    "                    self.translation_prob[source_word_idx, target_word_idx] = updated_prob\n",
    "\n",
    "            if self.change < self.tolerance:\n",
    "                self.has_converged = True\n",
    "            self.save(iteration)\n",
    "\n",
    "        if self.has_converged:\n",
    "            logging.info(f\"Model converged after {iteration} iteration under error {self.tolerance}!\")\n",
    "        else:\n",
    "            logging.info(f\"Training stopped after reaching max number of iterations {self.max_iterations}\")\n",
    "\n",
    "    def get_prob(self, target_word, source_word):\n",
    "        try:\n",
    "            target_index = self.target_tokenizer.toIndex[target_word]\n",
    "            source_index = self.source_tokenizer.toIndex[source_word]\n",
    "        except KeyError:\n",
    "            return -1\n",
    "        return self.translation_prob[source_index, target_index]\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "def align(model, model_r, bilingual_text):\n",
    "    \n",
    "    for (source_sentence, target_sentence) in bilingual_text:\n",
    "        alignment = []\n",
    "        alignment_r = []\n",
    "        for (source_index, source_word) in enumerate(source_sentence): \n",
    "            highest_probability = 0\n",
    "            best_target_index = 0\n",
    "            for (target_index, target_word) in enumerate(target_sentence):\n",
    "                probability = model.get_prob(target_word, source_word)\n",
    "                if probability > highest_probability:\n",
    "                    best_target_index = target_index\n",
    "                    highest_probability = probability\n",
    "            if(highest_probability > 0):\n",
    "                alignment.append([source_index,best_target_index])\n",
    "\n",
    "        for (target_index, target_word) in enumerate(target_sentence):\n",
    "            highest_probability = 0\n",
    "            best_index = 0\n",
    "            for (source_index, source_word) in enumerate(source_sentence):\n",
    "                probability = model_r.get_prob(source_word, target_word)\n",
    "                if probability >highest_probability:\n",
    "                    best_index = source_index\n",
    "                    highest_probability = probability\n",
    "            if(highest_probability > 0):\n",
    "                alignment_r.append([best_index,target_index])\n",
    "    \n",
    "        for i in range(len(source_sentence)):\n",
    "            for j in range(len(target_sentence)):\n",
    "                if [i, j] in alignment and [i, j] in alignment_r:\n",
    "                    sys.stdout.write(\"%i-%i \" % (i, j))\n",
    "\n",
    "        sys.stdout.write(\"\\n\")\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\": \n",
    "    o, f, e = parse()\n",
    "    loglvl = logging.DEBUG if o.debugging else logging.INFO\n",
    "    logging.basicConfig(level=loglvl)\n",
    "    b = [[s.strip().split() for s in p] for p in zip(open(f), open(e))][:o.num_sents]\n",
    "    br = [[s.strip().split() for s in p] for p in zip(open(e), open(f))][:o.num_sents]\n",
    "    if o.lower:\n",
    "        b = [[[word.lower() for word in fst],[word.lower() for word in est]] for fst, est in b]\n",
    "    ft = [sent[0] for sent in b]\n",
    "    et = [sent[1] for sent in b]\n",
    "    f_tokenizer = Tokenizer(ft)\n",
    "    e_tokenizer = Tokenizer(et)\n",
    "    \n",
    "    ftr = [sent[0] for sent in br]\n",
    "    etr = [sent[1] for sent in br]\n",
    "    f_tokenizer_r = Tokenizer(et)\n",
    "    e_tokenizer_r = Tokenizer(ft)\n",
    "\n",
    "  \n",
    "\n",
    "    logging.info(f\"french:{f_tokenizer.V}, english:{e_tokenizer.V}\")\n",
    "    if o.load_path:\n",
    "        ibm = pk.load(open(o.load_path, \"rb\"))\n",
    "        if o.iter_now is not None:\n",
    "            ibm.train(o.iter_now)\n",
    "    else:\n",
    "        ibm = IBMModel1(b, f_tokenizer, e_tokenizer, o.save_path, tolerance=o.eps, max_iterations=5)\n",
    "        ibm.train()\n",
    "        \n",
    "    if o.load_path_r:\n",
    "        ibm_r = pk.load(open(o.load_path_r, \"rb\"))\n",
    "        if o.iter_now is not None:\n",
    "            ibm_r.train(o.iter_now)\n",
    "    else:\n",
    "        ibm_r = IBMModel1(br, f_tokenizer_r, e_tokenizer_r, o.save_path_r, tolerance=o.eps, max_iterations=5)\n",
    "        ibm_r.train()\n",
    "\n",
    "    align(ibm, ibm_r, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a66e1ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
